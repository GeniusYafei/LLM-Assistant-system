services:
  frontend:
    build:
      context: ./frontend
    container_name: llm-chat-frontend
    ports:
      - "9900:9900"
    environment:
      - CHOKIDAR_USEPOLLING=true
    env_file:
      - ./frontend/.env
    volumes:
      - ./frontend:/app
      - frontend_node_modules:/app/node_modules
    command: npm run dev -- --host 0.0.0.0 --port 9900
    networks:
      - lorgan-net

  backend:
    build:
      context: ./backend
    container_name: llm-chat-backend
    restart: always
    env_file:
      - ./backend/.env
    environment:
      MOCK_LLM_BASE_URL: http://mock-llm:5000
      # Route Ollama requests to the host machine where the daemon listens
      OLLAMA_BASE_URL: http://host.docker.internal:11434
      OLLAMA_MODEL: qwen3:4b
      OLLAMA_TIMEOUT: 60.0
    depends_on:
      - db
      - mock-llm
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - ./uploads:/app/storage/uploads
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - lorgan-net
    command: >
      uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

  db:
    image: postgres:16-alpine
    container_name: llm-chat-db
    restart: always
    environment:
      POSTGRES_DB: local_test_db_lorgan
      POSTGRES_USER: app_user
      POSTGRES_PASSWORD: appuserpass
    ports:
      - "15432:5432"
    volumes:
      - db_data:/var/lib/postgresql/data
      #      Local test db
      - ./backend/console.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - lorgan-net

  mock-llm:
    build:
      context: ./backend/mock_llm
      dockerfile: Dockerfile
    container_name: llm-chat-mock-llm
    networks:
      - lorgan-net

volumes:
  frontend_node_modules:
  db_data:

networks:
  lorgan-net:
    driver: bridge
